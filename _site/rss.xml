<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
        <title>XGK&#39;s Tech Blog</title>
        <description>XGK&#39;s Tech Blog - Pengcheng He</description>
        <link>http://xgk.github.io</link>
        <atom:link href="http://xgk.github.io/rss.xml" rel="self" type="application/rss+xml" />
        <lastBuildDate>Sat, 06 Feb 2016 09:47:17 +0000</lastBuildDate>
        <pubDate>Sat, 06 Feb 2016 09:47:17 +0000</pubDate>
        <ttl>60</ttl>


        <item>
                <title>Notes on 1bit SGD</title>
                <description>&lt;p&gt;Recently I studied how &lt;a href=&quot;https://cntk.codeplex.com&quot;&gt;CNTK&lt;/a&gt; implemented the &lt;a href=&quot;http://research.microsoft.com/apps/pubs/?id=230137&quot;&gt;1-Bit SGD&lt;/a&gt; in sync mode. 
Here is a note  of the implementation.&lt;/p&gt;

&lt;p&gt;Most of the implementation is in files, &lt;code&gt;1BitSGD\AllReduceDistGradAggregator.h&lt;/code&gt; &amp;amp; &lt;code&gt;1BitSGD\MatrixQuantizer.h&lt;/code&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;background&quot;&gt;1.  Background&lt;/h4&gt;

&lt;p&gt;The Gradients of each layer in CNTK is stored as a &lt;strong&gt;Maxtrix of MxN&lt;/strong&gt;, e.g. &lt;em&gt;M inputs, N outputs&lt;/em&gt;. And the quantization of gradients is &lt;em&gt;based on each column&lt;/em&gt;, e.g. each output.&lt;/p&gt;

&lt;p&gt;There are several steps during distributed aggregation,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Encoding local sub-gradients of each &lt;strong&gt;Mini-Batch&lt;/strong&gt; using &lt;strong&gt;1 Bit&lt;/strong&gt; by quantization.&lt;/li&gt;
  &lt;li&gt;Send local quantized sub-gradients to other computation node in &lt;em&gt;striped&lt;/em&gt; mode to get aggregated.&lt;/li&gt;
  &lt;li&gt;Receive a strip of quantized sub-gradients from other nodes.&lt;/li&gt;
  &lt;li&gt;Decoding each received strip of quantized sub-gradients by unquantization, then &lt;strong&gt;add the unquantized results to aggregated gradients strip&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Encoding the strip of aggregated gradients using 1 bits by quantization.&lt;/li&gt;
  &lt;li&gt;Broadcast the quantized aggregated gradients strip to other nodes.&lt;/li&gt;
  &lt;li&gt;Receiving the quantized aggregated gradients strips from other nodes, and merge them to local quantized gradient matrix.&lt;/li&gt;
  &lt;li&gt;Decoding the aggregated local quantized gradient matrix as an approximation of the global gradients.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;quantizationunquantization&quot;&gt;2.  Quantization&amp;amp;UnQuantization&lt;/h4&gt;

&lt;p&gt;The quantization is based on each column. E.g.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Q = 0 if V&amp;gt;=0
Q = 1 if V&amp;lt;0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The max and min values of that column are stored together with the quantized results. See, &lt;code&gt;QuantizedColumn&lt;/code&gt;
The unquantization will use the max and min values of that column, e.g. (See &lt;code&gt;ValueQuantizer&lt;/code&gt;)
&lt;script type=&quot;math/tex&quot;&gt;V’ = (Q+0.5)*(max-min)/2 + min;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;strip&quot;&gt;3.  Strip&lt;/h4&gt;

&lt;p&gt;A strip is a group of columns in the gradient matrix, which is decided by the number of distributed computation nodes. 
For example, suppose there are 4 nodes, and 8 columns, then each nodes will have a strip of 2 columns.&lt;/p&gt;

&lt;h4 id=&quot;residual&quot;&gt;4.  Residual&lt;/h4&gt;

&lt;p&gt;The residual of each column is calculated during quantization and used for the quantization of next mini-batch by adding it to the local sub-gradients of that column.&lt;/p&gt;

&lt;h4 id=&quot;an-example-of-message-exchange&quot;&gt;5.  An example of message exchange&lt;/h4&gt;

&lt;p&gt;Here is an example picture of the message exchange during aggregation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Example_of_1Bit_SGD_Message_Pass.png&quot; alt=&quot;Example of 1Bit SGD Message Exchange&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Node0&lt;/em&gt; will collect quantized local-sub-gradients of &lt;em&gt;Strip0&lt;/em&gt; from all other 3 &lt;em&gt;nodes&lt;/em&gt;. 
Then unquantizes each of them and add results to aggregated results of &lt;em&gt;strip0&lt;/em&gt;.
Then quantize the aggregated results of &lt;em&gt;Strip0&lt;/em&gt;, e.g. &lt;em&gt;AggStrip0&lt;/em&gt; and broadcast it to other 3 &lt;em&gt;nodes&lt;/em&gt;.&lt;/p&gt;

</description>
                <link>http://xgk.github.io/machine%20learning/2016/02/06/1Bit</link>
                <guid>http://xgk.github.io/machine%20learning/2016/02/06/1Bit</guid>
                <pubDate>Sat, 06 Feb 2016 00:00:00 +0000</pubDate>
        </item>

        <item>
                <title>Notes on RDMA</title>
                <description>
&lt;p&gt;&lt;em&gt;RDMA&lt;/em&gt; 
是远程直接内存访问的缩写。最开使用在Infiniban技术中，但是由于Inifiband技术成本高，推广应用不好。
随着RoCE技术的出现使得RDMA应用到Ethernet上成为可能。RDMA相比于 TCP／IP over ethernet的好处在于，&lt;/p&gt;

&lt;!--more--&gt;

&lt;ol&gt;
  &lt;li&gt;直接写远程主机开辟出来的内存地址，绕过操作系统的TCP/IP socket协议栈。避免了通信过程中的中断开销和操作系统
内核切换的开销。从而释放出部分CPU资源用于计算，这一点在数据传输量很大的情况下比较明显，比如40Gbps&lt;/li&gt;
  &lt;li&gt;没有TCP链接建立过程的中的握手开销，有利于高频短报文的传输&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;当然RDMA相对于传统的TCP/IP over ethernet而言，也有一些缺陷，比如&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;需要专门的网卡和交换机支持。虽然这套RDMA技术目前已经很成熟，但并不是所有的网卡和交换机都能够无缝支持RDMA。
有的需要专门配置，有的根本就不支持。&lt;/li&gt;
  &lt;li&gt;因为网卡需要为每个连接维护一个内存映射表，而这个表一般需要存储在网卡的有限的Cache中，否则性能将大打折扣。
因此单机允许的并发连接数有限，论文里面支持的网络规模一般在100台左右。&lt;/li&gt;
  &lt;li&gt;编程模型复杂，因为没有协议栈的支持，数据读写的协议全部要靠应用程序实现。目前已经有一些技术开始用TCP/IP的socket
接口来封装RDMA，从而简化RDMA的编程。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;虽然RDMA有这些缺点，但是随着数据中心内部主机带宽的快速增加，近几年有的已经达到40Gbps，TCP／IP over Ethernet
很难将这个带宽填满，已经十几年历史的RDMA又开始受到学术届和工业界的重视。&lt;/p&gt;

</description>
                <link>http://xgk.github.io/cloud%20computing/2016/02/01/rdma</link>
                <guid>http://xgk.github.io/cloud%20computing/2016/02/01/rdma</guid>
                <pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate>
        </item>

        <item>
                <title>Notes on Paxos</title>
                <description>
&lt;p&gt;最近读了一些&lt;em&gt;Paxos&lt;/em&gt;的文章. 对其总算有了一些初步的认识. 首先弄清楚Paxos要解决的问题，
  &lt;!--more--&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Leader失效后的再选举问题
    如果原来的Leader失效，多个Leader同时出现，并且同时提交事务，3PC无法解决结果一致性问题。2PC则直接导致服务不可用。Paxos通过引入提议序列号这样的逻辑时钟，解决了提交事务的冲突，最终达到结果一致。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cohort部分失效服务仍然可用并且恢复后保持与leader同步&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;
</description>
                <link>http://xgk.github.io/cloud%20computing/2016/01/27/Paxos</link>
                <guid>http://xgk.github.io/cloud%20computing/2016/01/27/Paxos</guid>
                <pubDate>Wed, 27 Jan 2016 00:00:00 +0000</pubDate>
        </item>

        <item>
                <title>Page One</title>
                <description>
&lt;p&gt;Hello world!&lt;/p&gt;

&lt;p&gt;This is my first blog on GitHub! \( f(x_i, W) =  W x_i \)&lt;/p&gt;

&lt;p&gt;Concretely, recall that the linear function had the form \( f(x_i,W) = W x_{ {i_1} ^2} \) and the SVM we developed was formulated as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \right] + \alpha R(W)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p = y_i&lt;/script&gt;

&lt;!--more--&gt;
</description>
                <link>http://xgk.github.io/cloud%20computing/2014/05/28/page-one</link>
                <guid>http://xgk.github.io/cloud%20computing/2014/05/28/page-one</guid>
                <pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate>
        </item>


</channel>
</rss>
