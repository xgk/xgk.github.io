---
layout: post
title: "Notes on RDMA"
description: ""
category: "Cloud Computing" 
tags: [ RDMA, Network ]
---
{% include JB/setup %}

*RDMA* 
是远程直接内存访问的缩写。最开使用在Infiniban技术中，但是由于Inifiband技术成本高，推广应用不好。
随着RoCE技术的出现使得RDMA应用到Ethernet上成为可能。RDMA相比于 TCP／IP over ethernet的好处在于，

<!--more-->

1. 直接写远程主机开辟出来的内存地址，绕过操作系统的TCP/IP socket协议栈。避免了通信过程中的中断开销和操作系统
内核切换的开销。从而释放出部分CPU资源用于计算，这一点在数据传输量很大的情况下比较明显，比如40Gbps
2. 没有TCP链接建立过程的中的握手开销，有利于高频短报文的传输

当然RDMA相对于传统的TCP/IP over ethernet而言，也有一些缺陷，比如

1. 需要专门的网卡和交换机支持。虽然这套RDMA技术目前已经很成熟，但并不是所有的网卡和交换机都能够无缝支持RDMA。
有的需要专门配置，有的根本就不支持。
2. 因为网卡需要为每个连接维护一个内存映射表，而这个表一般需要存储在网卡的有限的Cache中，否则性能将大打折扣。
因此单机允许的并发连接数有限，论文里面支持的网络规模一般在100台左右。
3. 编程模型复杂，因为没有协议栈的支持，数据读写的协议全部要靠应用程序实现。目前已经有一些技术开始用TCP/IP的socket
接口来封装RDMA，从而简化RDMA的编程。

虽然RDMA有这些缺点，但是随着数据中心内部主机带宽的快速增加，近几年有的已经达到40Gbps，TCP／IP over Ethernet
很难将这个带宽填满，已经十几年历史的RDMA又开始受到学术届和工业界的重视。


